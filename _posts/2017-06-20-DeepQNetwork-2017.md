---
layout:     post
title:      "Deep Reinforcement Learning Research"
subtitle:   "给自己续一秒"
date:       2017-06-20 20:25:00
author:     "UnrealBook"
header-img: "img/invoker.jpg"
tags:
    - 学习
---


## Deep Reinforcement Learning Research

 **UnrealBook阅读并整理的相关资料**

** 1.what is deep reinforcement learning **
　　深度强化学习是将深度学习与强化学习结合起来从而实现从感知到动作的端对端学习（End-to-End Learning）的一种全新的算法。简单的说，就是和人类一样，输入感知信息比如视觉，然后通过深度神经网络，直接输出动作，中间没有hand-crafted engineering的工作。深度强化学习具备使机器人实现真正完全自主的学习一种甚至多种技能的潜力，最典型的应用就是最近很火的google的AlphaGo。
　　将深度学习和强化学习结合的想法在几年前就有人尝试，但真正成功的开端就是DeepMind在NIPS 2013上发表的[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)一文，在该文中第一次提出Deep Reinforcement Learning 这个名称，并且提出DQN（Deep Q-Network）算法，实现从纯图像输入完全通过学习来玩Atari游戏的成果。之后DeepMind在Nature上发表了改进版的DQN文章Human-level Control through Deep Reinfor-cement Learning，引起了广泛的关注，Deep Reinfocement Learning 从此成为深度学习领域的前沿研究方向。
  
 **强化学习**
　　强化学习也称增强学习，强调如何基于环境而行动，以获取最大化的预期利益，其灵感是来源于心理学中的行为学理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。强化学习的问题都可以模型化为MDP（马尔可夫决策过程）的问题。一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示状态转移概率，也就是根据当前的状态s<sub>t</sub>和a<sub>t</sub>转移到s<sub>t+1</sub>的概率。如果我们知道了转移概率P，也就是称为我们获得了模型Model，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为Model-based的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有Model-free的方法来寻找最优的动作。
  
** 2.what is DQN**
　　DQN是基于value function的算法，计算价值是总回报（Return）的期望，用G<sub>t</sub>表示t时刻的回报：
<img src="http://chart.googleapis.com/chart?cht=tx&chl=G_t%3DR_%7Bt%2B1%7D%2B%5Clambda%20R_%7Bt%2B2%7D%2B...%3D%5Csum_%7Bk%3D0%7D%5E%5Cinfty%5Clambda%5EkR_%7Bt%2Bk%2B1%7D" style="border:none;" />
　　上面R是Reward反馈，λ是discount factor折扣因子，一般小于1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。计算价值（value function）:
<img src="http://chart.googleapis.com/chart?cht=tx&chl=v(s)%3DE%5BG_t%7CS_t%3Ds%5D" style="border:none;" />
　　对于获取最优的策略Policy这个目标，我们通常有两种方法：

- 直接优化策略<img src="http://chart.googleapis.com/chart?cht=tx&chl=%5Cpi%20(a%7Cs)" style="border:none;" />或者<img src="http://chart.googleapis.com/chart?cht=tx&chl=a%3D%5Cpi%20(s)" style="border:none;" />使得回报更高
- 通过估计value function来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。

　　在[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)一文中，作者提出David Sliver分析了实现深度学习与增强学习相结合存在的三个问题：

- 没有标签怎么办？
- 样本的相关性太高怎么办？
- 目标分布不固定怎么办？

　　针对前两个问题，文章中给出了比较完美的解决方案，1.首先没有标签怎么办？因为深度学习是依赖于大量的有标签数据的样本从而进行监督学习，而强化学习只有一个Reward返回值，而且这个值还伴有噪音、延迟等。DQN模型中提出了通过Q-Learning来构造标签；

　　Q网络的损失函数：<img src="http://chart.googleapis.com/chart?cht=tx&chl=L_i(%5Ctheta%20)%3DE_%7Bs%2Ca%5Csim%5Crho%20(%5Ccdot)%20%5B(y_i-Q(s%2Ca%3BQ_i))%5E2%5D" style="border:none;" />　s,a即下一个状态与动作；

　　价值函数：<img src="http://chart.googleapis.com/chart?cht=tx&chl=Q%5E*(s%2Ca)%3DE_%7Bs%5E'%5Csim%5Cepsilon%20%7D%5Br%2B%5Cgamma%20%5Cunderset%7Ba%5E'%7D%7Bmax%7DQ%5E*(s%5E'%2Ca%5E')%7Cs%2Ca%5D" style="border:none;" />

　　Q-Learning算法其实就是使用贝尔曼方程来求解一个马尔可夫决策过程问题（MDP），最后要最优化行动价值函数（这里不详细介绍了，感兴趣的话可以阅读知乎上[Flood Sung](https://www.zhihu.com/people/flood-sung/answers)写的[DQN从入门到放弃系列文章](https://zhuanlan.zhihu.com/p/21262246)）；Q-Learning算法更新Q值就是依靠利用Reward和Q来计算出目标Q值：<img src="http://chart.googleapis.com/chart?cht=tx&chl=R_%7Bt%2B1%7D%2B%5Clambda%20%5Cunderset%7Ba%7D%7Bmax%7DQ(S_%7Bt%2B1%7D%2Ca)" style="border:none;" />；因此，将目标Q值作为标签，让我们的目标不断趋近于目标Q值就行了。2.通过Experience replay（即经验池） 的方法来解决相关性及非静态分布问题，以Atari采集样本为例，样本之间具有连续性，如果每次得倒样本就更新Q值，会受样本分布影响。因此一个直接方法是将样本先存储在一个记忆容器中，然后再采用随机梯度下降（SDG）方法来训练样本更新权值最后达到训练网络，输出最优决策的目标。也就是说在DQN中强化学习Q-Learning与深度学习的SDG训练是同步进行的。

　　DQN算法：

![Aaron Swartz](https://raw.githubusercontent.com/UnrealBook/UnrealBlog/master/img/DQN.png)

　　算法分析： 

- 1. 训练分成M个episode，每个episode训练T次。我的理解就是比如玩游戏，一局是一个episode，一局里面有很多时间片，就训练多少次，次数不固定。重启新的episode主要是初始化state 作为新的第一个，而不是用上一局的最后的状态作为state输入。 
- 2. 实际上每个循环分成两部分：一部分是输出动作并存储。一部分是随机从经验池里取出minibatch个transitions，然后计算target，根据loss function通过RMSProp更新参数。（minibatch是什么意思？） 
- 3. 这里的算法我们可以看到，参数是一直更新的，而Nature的算法改进了，计算target用的是之前的参数。具体算法的变化等之后分析Nature的文章再说。

　　DQN模型：

![Aaron Swartz](https://raw.githubusercontent.com/UnrealBook/UnrealBlog/master/img/DQN_Model.png)

　　输入是经过处理的4个连续的84x84图像，然后经过两个卷积层，两个全连接层，最后输出包含每一个动作Q值的向量。总之，用神经网络来表示Q值非常简单，Q值也就是变成用Q网络（Q-Network）来表示。通过上述的DQN算法训练即可得到我们想要的结果。

** 3.Conclusion**
 　　在NIPS13这篇论文之后DeepMind不断对DQN进行改进，首先在2015年初发布了Nature文章，提出了 Nature版本的DQN，然后接下来在2015年一年内提出了Double DQN，Prioritied Replay，还有Dueling Network三种主要方法，又极大的提升了DQN的性能，目前的改进型DQN算法在Atari游戏的平均得分是Nature版DQN的三倍之多。


**声明：**本文并非原创博客=￣ω￣=，主要摘自[知乎Flood Sung DQN 从入门到放弃系列文章](https://zhuanlan.zhihu.com/p/21262246)和[ Paper Reading 1 - Playing Atari with Deep Reinforcement Learning](http://blog.csdn.net/songrotek/article/details/50581011)真的特别欣赏这种勤于积累、乐于分享知识的大神，本来只是自己做的关于DRL方向的小调研，并不想写成博客的，不过最近真的是太懒了，好久没怎么学习了，虽然没什么干货，但还是逼着自己写点东西当作一个积累和鞭笞吧！以后争取多写点有价值的原创博客！最后谢谢您的到来！祝生活愉快！(^_^)
